{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMpkZDmWEFIk1hnCw8z/Yd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bida22-070/BIDA22-070/blob/main/Week_5_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tuAz86bn9IH",
        "outputId": "933d3fe1-f900-4bf7-a3c9-2287bb0bbbcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exercise 2-3: Installing Required Libraries\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading NLTK datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK setup completed!\n",
            "\n",
            "Exercise 4: Initial Setup and Sample Text\n",
            "Original Text: \n",
            "This is a simple example of text preprocessing. It involves cleaning and organizing raw text data into a format suitable for analysis.\n",
            "\n",
            "Exercise 5: Convert to Lowercase\n",
            "Lower case Text: \n",
            "this is a simple example of text preprocessing. it involves cleaning and organizing raw text data into a format suitable for analysis.\n",
            "\n",
            "Exercise 6: Remove Punctuation\n",
            "Removed punctuation: \n",
            "this is a simple example of text preprocessing it involves cleaning and organizing raw text data into a format suitable for analysis\n",
            "\n",
            "Exercise 7: Tokenization\n",
            "All tokens: \n",
            "['this', 'is', 'a', 'simple', 'example', 'of', 'text', 'preprocessing', 'it', 'involves', 'cleaning', 'and', 'organizing', 'raw', 'text', 'data', 'into', 'a', 'format', 'suitable', 'for', 'analysis']\n",
            "\n",
            "Exercise 8: Remove Stopwords\n",
            "After Text Preprocessing: \n",
            "['simple', 'example', 'text', 'preprocessing', 'involves', 'cleaning', 'organizing', 'raw', 'text', 'data', 'format', 'suitable', 'analysis']\n",
            "\n",
            "Exercise 9: Testing with Different Sentences\n",
            "Sentence 1: Natural Language Processing is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language.\n",
            "Processed: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'computers', 'human', 'language']\n",
            "\n",
            "Sentence 2: Machine learning algorithms can analyze and understand textual data to perform tasks like sentiment analysis, text classification, and language translation.\n",
            "Processed: ['machine', 'learning', 'algorithms', 'analyze', 'understand', 'textual', 'data', 'perform', 'tasks', 'like', 'sentiment', 'analysis', 'text', 'classification', 'language', 'translation']\n",
            "\n",
            "Sentence 3: The University of Sunderland offers excellent courses in Computer Science and Artificial Intelligence that prepare students for careers in technology.\n",
            "Processed: ['university', 'sunderland', 'offers', 'excellent', 'courses', 'computer', 'science', 'artificial', 'intelligence', 'prepare', 'students', 'careers', 'technology']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2-3: Install required libraries\n",
        "print(\"Exercise 2-3: Installing Required Libraries\")\n",
        "!pip install nltk scikit-learn\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data\n",
        "print(\"Downloading NLTK datasets...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "print(\"NLTK setup completed!\\n\")\n",
        "\n",
        "# Exercise 4: Initialize and sample text\n",
        "print(\"Exercise 4: Initial Setup and Sample Text\")\n",
        "text = \"This is a simple example of text preprocessing. It involves cleaning and organizing raw text data into a format suitable for analysis.\"\n",
        "print(\"Original Text: \")\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "# Exercise 5: Convert to lowercase\n",
        "print(\"Exercise 5: Convert to Lowercase\")\n",
        "text_lower = text.lower()\n",
        "print(\"Lower case Text: \")\n",
        "print(text_lower)\n",
        "print()\n",
        "\n",
        "# Exercise 6: Remove punctuation\n",
        "print(\"Exercise 6: Remove Punctuation\")\n",
        "text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Removed punctuation: \")\n",
        "print(text_no_punct)\n",
        "print()\n",
        "\n",
        "# Exercise 7: Tokenization\n",
        "print(\"Exercise 7: Tokenization\")\n",
        "tokens = word_tokenize(text_no_punct)\n",
        "print(\"All tokens: \")\n",
        "print(tokens)\n",
        "print()\n",
        "\n",
        "# Exercise 8: Remove stopwords\n",
        "print(\"Exercise 8: Remove Stopwords\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(\"After Text Preprocessing: \")\n",
        "print(filtered_tokens)\n",
        "print()\n",
        "\n",
        "# Exercise 9: Test with different sentences\n",
        "print(\"Exercise 9: Testing with Different Sentences\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text_no_punct)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"Natural Language Processing is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language.\",\n",
        "    \"Machine learning algorithms can analyze and understand textual data to perform tasks like sentiment analysis, text classification, and language translation.\",\n",
        "    \"The University of Sunderland offers excellent courses in Computer Science and Artificial Intelligence that prepare students for careers in technology.\"\n",
        "]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n",
        "    processed = preprocess_text(sentence)\n",
        "    print(f\"Processed: {processed}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 10: Create and read text file\n",
        "print(\"Exercise 10: Create and Read Text File\")\n",
        "\n",
        "# Create a text file\n",
        "sample_text = \"\"\"Artificial Intelligence is transforming the world around us.\n",
        "Machine learning algorithms are being used in various industries including healthcare, finance, and transportation.\n",
        "Natural Language Processing enables computers to understand and interpret human language.\n",
        "AI systems can analyze large amounts of data and make predictions with high accuracy.\n",
        "The field of AI continues to evolve rapidly with new breakthroughs happening regularly.\n",
        "Researchers are working on developing more advanced AI models that can solve complex problems.\n",
        "Ethical considerations in AI development are becoming increasingly important as the technology advances.\"\"\"\n",
        "\n",
        "# Write to file\n",
        "with open('ai_paragraph.txt', 'w') as file:\n",
        "    file.write(sample_text)\n",
        "\n",
        "# Read from file\n",
        "with open('ai_paragraph.txt', 'r') as file:\n",
        "    file_content = file.read()\n",
        "\n",
        "print(\"Content read from file:\")\n",
        "print(file_content)\n",
        "print()\n",
        "\n",
        "# Exercise 11: Apply preprocessing to file content\n",
        "print(\"Exercise 11: Preprocess File Content\")\n",
        "processed_file_content = preprocess_text(file_content)\n",
        "print(\"Processed file content:\")\n",
        "print(processed_file_content)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pu5ABNcu_xN",
        "outputId": "78067f03-89cf-40c5-9cbe-a346e67284df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exercise 10: Create and Read Text File\n",
            "Content read from file:\n",
            "Artificial Intelligence is transforming the world around us. \n",
            "Machine learning algorithms are being used in various industries including healthcare, finance, and transportation. \n",
            "Natural Language Processing enables computers to understand and interpret human language. \n",
            "AI systems can analyze large amounts of data and make predictions with high accuracy. \n",
            "The field of AI continues to evolve rapidly with new breakthroughs happening regularly. \n",
            "Researchers are working on developing more advanced AI models that can solve complex problems. \n",
            "Ethical considerations in AI development are becoming increasingly important as the technology advances.\n",
            "\n",
            "Exercise 11: Preprocess File Content\n",
            "Processed file content:\n",
            "['artificial', 'intelligence', 'transforming', 'world', 'around', 'us', 'machine', 'learning', 'algorithms', 'used', 'various', 'industries', 'including', 'healthcare', 'finance', 'transportation', 'natural', 'language', 'processing', 'enables', 'computers', 'understand', 'interpret', 'human', 'language', 'ai', 'systems', 'analyze', 'large', 'amounts', 'data', 'make', 'predictions', 'high', 'accuracy', 'field', 'ai', 'continues', 'evolve', 'rapidly', 'new', 'breakthroughs', 'happening', 'regularly', 'researchers', 'working', 'developing', 'advanced', 'ai', 'models', 'solve', 'complex', 'problems', 'ethical', 'considerations', 'ai', 'development', 'becoming', 'increasingly', 'important', 'technology', 'advances']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}